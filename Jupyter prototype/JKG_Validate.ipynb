{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c276accd-a97b-41f1-9c30-af99bbaf5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this notebook is to Validate JSON Knowledge Graph against schema, uniqueness, and referential integrity.\n",
    "# Each cell is generally recommended to use to validate patterns.\n",
    "# Each cell will work for millions of nodes and rels, but if each node and rel are generated then is validation needed for each to SCHEMA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b9cddcc-9751-47cd-88d1-6e76ac79842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python3 modules\n",
    "import os\n",
    "import json as json\n",
    "import jsonschema as jsonschema\n",
    "import pandas as pd\n",
    "from loky import get_reusable_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77393e7a-fcff-48d4-98e9-8d3b2883581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON KG and JKG Schema from local directory - more than 1 min per GB to load \n",
    "with open('JKG.json') as file: JKG = json.load(file)\n",
    "with open('JKG_Schema.json') as file: JKG_Schema = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22d8551c-dfd7-4bfb-9abc-e1d5d9029d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide JKG_Schema into top level and sub-schemas for parallel processing\n",
    "nodes_schema = JKG_Schema['properties'].pop('nodes')\n",
    "rels_schema = JKG_Schema['properties'].pop('rels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b707a0b4-a306-4a37-a221-1b3749b1d43b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JKG data has nodes and rels lists.\n"
     ]
    }
   ],
   "source": [
    "# Validate JKG against top level of JKG_Schema\n",
    "try:\n",
    "    jsonschema.validate(JKG,JKG_Schema)\n",
    "    print(\"JKG data has nodes and rels lists.\")\n",
    "except jsonschema.exceptions.ValidationError as e:\n",
    "    print(f\"JKG data is INVALID: {e.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f11260f-df9e-45cd-a996-cce2ffc4661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema validation begins for nodes 0 to 11025251\n",
      "In each 1000 nodes up to one INVALID node is flagged.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Future at 0x5a4000070 state=pending>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate node items to SCHEMA in parallel ~ one million nodes runs 3.5 minutes on Apple M1 Max with 32GB Memory\n",
    "# YOU MAY NOT WANT TO RUN THIS CELL for 10 MILLIONS of Nodes\n",
    "def validate_items(items,s,f):\n",
    "    try:\n",
    "        jsonschema.validate(items,nodes_schema)\n",
    "        #print(f\"Processed successfully nodes: {s} to {f}\")\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        print(f\"Processing nodes: {s} to {f}\")\n",
    "        print(f\"INVALID: node row: {e.json_path}, {e.message}\")\n",
    "\n",
    "max_index = len(JKG['nodes'])-1\n",
    "executor = get_reusable_executor(max_workers=10, timeout=3)\n",
    "print(f\"Schema validation begins for nodes 0 to {max_index}\")\n",
    "print(\"In each 1000 nodes up to one INVALID node is flagged.\")\n",
    "for i in range(int(max_index / 1000)):\n",
    "    s = i*1000\n",
    "    f = s+1000\n",
    "    executor.submit(validate_items,JKG['nodes'][s:f],s,f)\n",
    "s = int(max_index / 1000) * 1000\n",
    "f = s + (max_index % 1000)\n",
    "executor.submit(validate_items,JKG['nodes'][s:f],s,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7a9a44-b517-46ce-982d-d13916f4051c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema validation begins for rels 0 to 21336318\n",
      "In each 1000 rels up to one INVALID rel is flagged.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Future at 0x755c679fd0 state=pending>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate rel items to SCHEMA in parallel ~ one million rels runs 3.5 minutes on Apple M1 Max with 32GB Memory\n",
    "# YOU MAY NOT WANT TO RUN THIS CELL for 10 MILLIONS of Rels\n",
    "def validate_items(items,s,f):\n",
    "    try:\n",
    "        jsonschema.validate(items,rels_schema)\n",
    "        #print(f\"Processed successfully rels: {s} to {f}\")\n",
    "    except jsonschema.exceptions.ValidationError as e:\n",
    "        print(f\"Processing rels: {s} to {f}\")\n",
    "        print(f\"INVALID: rel row: {e.json_path}, {e.message}\")\n",
    "\n",
    "max_index = len(JKG['rels'])-1\n",
    "executor = get_reusable_executor(max_workers=10, timeout=3)\n",
    "print(f\"Schema validation begins for rels 0 to {max_index}\")\n",
    "print(\"In each 1000 rels up to one INVALID rel is flagged.\")\n",
    "for i in range(int(max_index / 1000)):\n",
    "    s = i*1000\n",
    "    f = s+1000\n",
    "    executor.submit(validate_items,JKG['rels'][s:f],s,f)\n",
    "s = int(max_index / 1000) * 1000\n",
    "f = s + (max_index % 1000)\n",
    "executor.submit(validate_items,JKG['rels'][s:f],s,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fba4ead-6489-4bda-8485-374eaf40500e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JOS220/Library/Python/3.9/lib/python/site-packages/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "IOStream.flush timed out\n",
      "/Users/JOS220/Library/Python/3.9/lib/python/site-packages/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "IOStream.flush timed out\n",
      "/Users/JOS220/Library/Python/3.9/lib/python/site-packages/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load rels and nodes into Data Frames\n",
    "rels = pd.DataFrame(JKG['rels'])\n",
    "nodes = pd.DataFrame(JKG['nodes'])\n",
    "\n",
    "df = pd.json_normalize(rels)\n",
    "starts = pd.json_normalize(rels.start)\n",
    "ends = pd.json_normalize(rels.end)\n",
    "df = pd.json_normalize(rels.properties)\n",
    "df = pd.concat([rels.label.reset_index(drop=True),df.reset_index(drop=True)], axis=1)\n",
    "rels = df\n",
    "\n",
    "df = pd.json_normalize(nodes.properties)\n",
    "df = pd.concat([nodes.labels.reset_index(drop=True),df.reset_index(drop=True)], axis=1)\n",
    "nodes = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ab3bbe0-1d4e-4b9c-9aac-c40f6d7da20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All node id are unique.\n",
      "All Source sab are unique.\n",
      "All Node_Label node_label are unique.\n",
      "All Rel_Label rel_label are unique.\n"
     ]
    }
   ],
   "source": [
    "# Validate Uniqueness of node ids, sabs, node_labels, rel_labels\n",
    "# df is nodes carried forward from last notebook cell\n",
    "\n",
    "# Check duplicate node id\n",
    "duplicates = df[df.duplicated(subset=['id'], keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(\"The following node id are not unique:\\n\", duplicates)\n",
    "else:\n",
    "    print(\"All node id are unique.\")\n",
    "    \n",
    "# Subset nodes to Source and Check duplicate sab\n",
    "fdf = df[df['labels'].apply(lambda x: 'Source' in x)]\n",
    "duplicates = fdf[fdf.duplicated(subset=['sab'], keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(\"The following Source sab are not unique:\\n\", duplicates)\n",
    "else:\n",
    "    print(\"All Source sab are unique.\")\n",
    "    \n",
    "# Subset nodes to Node_Label and Check duplicate node_label\n",
    "fdf = df[df['labels'].apply(lambda x: 'Node_Label' in x)]\n",
    "duplicates = fdf[fdf.duplicated(subset=['node_label'], keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(\"The following Node_Label node_label are not unique:\\n\", duplicates)\n",
    "else:\n",
    "    print(\"All Node_Label node_label are unique.\")\n",
    "    \n",
    "# Subset nodes to Rel_Label and Check duplicate rel_label\n",
    "fdf = df[df['labels'].apply(lambda x: 'Rel_Label' in x)]\n",
    "duplicates = fdf[fdf.duplicated(subset=['rel_label'], keep=False)]\n",
    "if not duplicates.empty:\n",
    "    print(\"The following Rel_Label rel_label are not unique:\\n\", duplicates)\n",
    "else:\n",
    "    print(\"All Rel_Label rel_label are unique.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2bf671a-a8d3-434d-98f1-d287bfa0904c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Node sab are present in Source sab.\n",
      "All Rel sab are present in Source sab.\n",
      "All Concept Labels are present in node_label.\n",
      "All Rel labels are present in rel_label.\n",
      "All Rel start id are present in node id.\n",
      "The following Rel end id are not asserted as a node id:\n",
      "141160     UMLS:C0949778\n",
      "1428308    UMLS:C5234793\n",
      "1612469    UMLS:C4082610\n",
      "2436150    UMLS:C0949821\n",
      "3335648    UMLS:C4300557\n",
      "3812654    UMLS:C4082319\n",
      "4603689    UMLS:C0031324\n",
      "4635977    UMLS:C0949906\n",
      "5333716    UMLS:C2825126\n",
      "6198437    UMLS:C3179396\n",
      "7507312    UMLS:C0949835\n",
      "7544827    UMLS:C4082410\n",
      "8465206    UMLS:C0949781\n",
      "9163829    UMLS:C2825126\n",
      "9448240    UMLS:C2348859\n",
      "9616278    UMLS:C0949777\n",
      "Name: properties.id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Validate Referential Integrity\n",
    "# Check if all values in list_a are in list_b\n",
    "# missing_values = list_a[~list_a.isin(list_b)]\n",
    "\n",
    "# Reports Node sab NOT in Source sab list\n",
    "fdf = nodes[nodes['labels'].apply(lambda x: 'Source' in x)]\n",
    "u_sab = pd.Series(fdf['sab'])\n",
    "nodes_sab = pd.Series(nodes['sab'].unique()).dropna() # drop NaN because Term nodes have no sab\n",
    "missing_values = nodes_sab[~nodes_sab.isin(u_sab)]\n",
    "if not missing_values.empty:\n",
    "    print(f\"The following Node sab are not asserted as a Source sab:\\n{missing_values}\")\n",
    "else:\n",
    "    print(\"All Node sab are present in Source sab.\")\n",
    "\n",
    "# Reports Rel sab NOT in Source sab list - uses Source sab list u_sab from above\n",
    "rels_sab = pd.Series(rels['sab'].unique())\n",
    "missing_values = rels_sab[~rels_sab.isin(u_sab)]\n",
    "if not missing_values.empty:\n",
    "    print(f\"The following Rel sab are not asserted as a Source sab:\\n{missing_values}\")\n",
    "else:\n",
    "    print(\"All Rel sab are present in Source sab.\")\n",
    "\n",
    "# Reports Concept other Labels NOT in node_label list with Concept added\n",
    "fdf = nodes[nodes['labels'].apply(lambda x: 'Concept' in x)]\n",
    "u_labels = pd.Series(fdf['labels'].explode().unique())\n",
    "node_labels_concept = pd.concat([nodes.node_label, pd.Series(['Concept'])], ignore_index=True)\n",
    "missing_values = u_labels[~u_labels.isin(node_labels_concept)]\n",
    "if not missing_values.empty:\n",
    "    print(f\"The following Labels are not asserted as a node_label:\\n{missing_values}\")\n",
    "else:\n",
    "    print(\"All Concept Labels are present in node_label.\")\n",
    "\n",
    "# Reports Rel label NOT in rel_label list with CODE added\n",
    "rel_labels_CODE = pd.concat([nodes.rel_label, pd.Series(['CODE'])], ignore_index=True)\n",
    "u_labels = pd.Series(rels['label'].unique())\n",
    "missing_values = u_labels[~u_labels.isin(rel_labels_CODE)]\n",
    "if not missing_values.empty:\n",
    "    print(f\"The following Rel labels are not asserted as a rel_label:\\n{ missing_values}\")\n",
    "else:\n",
    "    print(\"All Rel labels are present in rel_label.\")\n",
    "\n",
    "# Reports start property.id of rels in node id list\n",
    "u_labels = starts['properties.id']\n",
    "missing_values = u_labels[~u_labels.isin(nodes.id)]\n",
    "if not missing_values.empty:\n",
    "    print(f\"The following Rel start id are not asserted as a node id:\\n{missing_values}\")\n",
    "else:\n",
    "    print(\"All Rel start id are present in node id.\")\n",
    "\n",
    "# Reports end property.id of rels in node id list\n",
    "u_labels = ends['properties.id']\n",
    "missing_values = u_labels[~u_labels.isin(nodes.id)]\n",
    "if not missing_values.empty:\n",
    "    print(f\"The following Rel end id are not asserted as a node id:\\n{missing_values}\")\n",
    "else:\n",
    "    print(\"All Rel end id are present in node id.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db662d-b447-4347-80e7-c0b7cfe64aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
